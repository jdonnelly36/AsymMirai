{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean start, trying to construct DF to match filtering criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centroid_activation(array, threshold=0.02):\n",
    "    print(array.shape)\n",
    "    h, w = array.shape\n",
    "    am = torch.argmax(array)\n",
    "    am_h, am_w = am // w, am % w\n",
    "    \n",
    "    # Grab all the locations at which activation is within threshold of the max\n",
    "    candidate_locations = list(((array[am_h, am_w] - array) <= threshold).nonzero())\n",
    "    \n",
    "    # First, we're going to grab all the locations that are contiguous with the max\n",
    "    added_new = True\n",
    "    contiguous_w_max = [torch.tensor([am_h, am_w])]\n",
    "    \n",
    "    while added_new:\n",
    "        added_new = False\n",
    "        to_move = []\n",
    "        for cl_ind, cl in enumerate(candidate_locations):\n",
    "            for contig_ind, contig in enumerate(contiguous_w_max):\n",
    "                if abs(cl[0] - contig[0]) <= 1 and abs(cl[1] - contig[1]) <= 1:\n",
    "                    if abs(cl[0] - contig[0]) == 0 and abs(cl[1] - contig[1]) == 0:\n",
    "                        continue\n",
    "                    if cl_ind not in to_move:\n",
    "                        to_move.append(cl_ind)\n",
    "                    added_new = True\n",
    "        \n",
    "        for index in sorted(to_move, reverse=True):\n",
    "            contiguous_w_max.append(candidate_locations[index])\n",
    "            del candidate_locations[index]\n",
    "            \n",
    "    # This is a bit of a hack, but the true max gets double counted,\n",
    "    # so this removes the first time we counted it\n",
    "    if len(contiguous_w_max) > 1:\n",
    "        del contiguous_w_max[0]\n",
    "        \n",
    "    h_mean, w_mean = 0.0, 0.0\n",
    "    for cm in contiguous_w_max:\n",
    "        h_mean += cm[0].item()\n",
    "        w_mean += cm[1].item()\n",
    "    print(h_mean, w_mean, contiguous_w_max)\n",
    "    h_mean /= len(contiguous_w_max)\n",
    "    w_mean /= len(contiguous_w_max)\n",
    "    \n",
    "    return (h_mean, w_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./asymmetry_model')\n",
    "from torch.utils.data import DataLoader\n",
    "from mirai_metadataset import MiraiMetadataset\n",
    "import torch.nn.functional as F\n",
    "from embed_explore import resize_and_normalize, crop\n",
    "import torch\n",
    "\n",
    "def resize_and_normalize(img, use_crop=False):\n",
    "    img_mean = 7699.5\n",
    "    img_std = 11765.06\n",
    "    target_size = (1664, 2048)\n",
    "    dummy_batch_dim = False\n",
    "\n",
    "    if np.sum(img) == 0:\n",
    "        img = torch.tensor(img).expand(1, 3, *img.shape)\\\n",
    "                        .type(torch.FloatTensor)\n",
    "        return F.upsample(img, size=(target_size[0], target_size[1]), mode='bilinear')[0]\n",
    "\n",
    "    # Adding a dummy batch dimension if necessary\n",
    "    if len(img.shape) == 3:\n",
    "        img = torch.unsqueeze(img, 0)\n",
    "        dummy_batch_dim = True\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if use_crop:\n",
    "            img = crop(torch.tensor((img - img_mean)/img_std))\n",
    "        else:\n",
    "            img = torch.tensor((img - img_mean)/img_std)\n",
    "        img = img.expand(1, 3, *img.shape)\\\n",
    "                        .type(torch.FloatTensor)\n",
    "        img_resized = F.upsample(img, size=(target_size[0], target_size[1]), mode='bilinear')\n",
    "    #img_resized = img\n",
    "\n",
    "    if dummy_batch_dim:\n",
    "        return img_resized[0]\n",
    "    else:\n",
    "        return img_resized[0]\n",
    "    \n",
    "def run_validation(model, val_df):\n",
    "    #'exam_id', 'prediction_neg', 'prediction_pos', 'y_argmin_cc',\n",
    "    #   'x_argmin_cc', 'y_argmin_mlo', 'x_argmin_mlo'\n",
    "    torch.cuda.set_device(6)\n",
    "    model = model.eval()\n",
    "    model.latent_h = 5\n",
    "    model.latent_w = 5\n",
    "    model.topk_for_heatmap = None\n",
    "    model.topk_weights = torch.tensor([1]).cuda()\n",
    "    model.use_bn = False\n",
    "    model.learned_asym_mean = model.initial_asym_mean\n",
    "    model.learned_asym_std = model.initial_asym_std\n",
    "\n",
    "\n",
    "    batch_size = 1\n",
    "\n",
    "    val_dataset = MiraiMetadataset(val_df, resizer=resize_and_normalize, mode='val', align_images=False, multiple_pairs_per_exam=False)#, use_crop=use_crop)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=min(10, batch_size))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        eids_for_epoch = []\n",
    "        centroids_h_cc_for_epoch = []\n",
    "        centroids_w_cc_for_epoch = []\n",
    "        centroids_h_mlo_for_epoch = []\n",
    "        centroids_w_mlo_for_epoch = []\n",
    "        predictions = []\n",
    "\n",
    "        for index, sample in enumerate(val_dataloader):\n",
    "\n",
    "            eid, label, l_cc_img, l_cc_path, r_cc_img, r_cc_path, l_mlo_img, l_mlo_path, r_mlo_img, r_mlo_path = sample\n",
    "            l_cc_img, r_cc_img, l_mlo_img, r_mlo_img = l_cc_img.cuda(), r_cc_img.cuda(), l_mlo_img.cuda(), r_mlo_img.cuda()\n",
    "            label = label.cuda()\n",
    "\n",
    "            output, other = model(l_cc_img, r_cc_img, l_mlo_img, r_mlo_img)\n",
    "            eids_for_epoch = eids_for_epoch + list(eid.numpy())\n",
    "            predictions = predictions + list(output.detach().cpu().numpy())\n",
    "            for c in range(2):\n",
    "                for i in range(batch_size):\n",
    "                    heatmap = other[c]['heatmap'][i]\n",
    "                    centroid = get_centroid_activation(heatmap)\n",
    "                    if c == 0:\n",
    "                        centroids_h_cc_for_epoch.append(centroid[0])\n",
    "                        centroids_w_cc_for_epoch.append(centroid[1])\n",
    "                    else:\n",
    "                        centroids_h_mlo_for_epoch.append(centroid[0])\n",
    "                        centroids_w_mlo_for_epoch.append(centroid[1])\n",
    "            df = pd.DataFrame({\n",
    "                'exam_id': eids_for_epoch,\n",
    "                'prediction_neg': list(1 - np.array(predictions)),\n",
    "                'prediction_pos': predictions,\n",
    "                'y_argmin_cc': centroids_h_cc_for_epoch,\n",
    "                'x_argmin_cc': centroids_w_cc_for_epoch,\n",
    "                'y_argmin_mlo': centroids_h_mlo_for_epoch,\n",
    "                'x_argmin_mlo': centroids_w_mlo_for_epoch\n",
    "            })\n",
    "            print(index)\n",
    "            df.to_csv('tmp_val_run.csv', index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "# This chunk comes from the dataset class itself, where some incomplete exams are excluded\n",
    "def get_incomplete_exams(metadata_frame):\n",
    "    incomplete_exams = []\n",
    "    for i, eid in tqdm.tqdm(enumerate(metadata_frame['exam_id'].unique()), total=metadata_frame['exam_id'].unique().shape[0]):\n",
    "\n",
    "        cur_exam = metadata_frame[metadata_frame['exam_id'].values == eid]\n",
    "\n",
    "        patient_exam = {'MLO': {'L': None, 'R': None},\n",
    "                        'CC': {'L': None, 'R': None}}\n",
    "        for view in patient_exam.keys():\n",
    "            def indices_for_side_view(side):\n",
    "                indices = np.logical_and(cur_exam['view'].values == view, cur_exam['laterality'].values == side)\n",
    "                return indices\n",
    "\n",
    "            if len(cur_exam[indices_for_side_view('L')]['file_path'].values) == 0:\n",
    "                incomplete_exams.append(eid)\n",
    "                continue\n",
    "            else:\n",
    "                for laterality in ['L', 'R']: \n",
    "                    if len(cur_exam[indices_for_side_view(laterality)]['file_path'].values) == 0:\n",
    "                        incomplete_exams.append(eid)\n",
    "                        continue\n",
    "                        \n",
    "    return incomplete_exams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Construct our dataset\n",
    "test_df = pd.read_csv('/PATH/TO/DATASET')\n",
    "print(test_df.columns)\n",
    "\n",
    "# Filter to 2d images\n",
    "filtered = test_df[test_df['FinalImageType'] == '2D']\n",
    "\n",
    "# And filter out those with missing views\n",
    "filtered['exam_id'] = filtered['acc_anon'].values\n",
    "filtered['patient_id'] = filtered['empi_anon_x'].values\n",
    "filtered['laterality'] = filtered['ImageLateralityFinal'].values\n",
    "filtered['view'] = filtered['ViewPosition'].values\n",
    "filtered['file_path'] = filtered['png_path'].values\n",
    "\n",
    "incomplete_exams = get_incomplete_exams(filtered)\n",
    "filtered = filtered[~filtered['exam_id'].isin(incomplete_exams)]\n",
    "print(filtered['patient_id'].unique().shape)\n",
    "print(filtered['exam_id'].unique().shape)\n",
    "\n",
    "# Finally, remove diagnostic exams\n",
    "filtered_input_df = filtered[filtered['desc'].str.contains('screen', case=False)]\n",
    "print(filtered_input_df['patient_id'].unique().shape)\n",
    "print(filtered_input_df['exam_id'].unique().shape)\n",
    "\n",
    "mirai_form_df = pd.read_csv('../2_10_mirai_form_extended_cohorts_1-2_with_matches.csv')\n",
    "filtered = filtered_input_df.join(mirai_form_df, lsuffix='', rsuffix='_mirai', on='exam_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mirai_form_df = pd.read_csv('../2_10_mirai_form_extended_cohorts_1-2_with_matches.csv')\n",
    "filtered = filtered_input_df.merge(mirai_form_df, on='file_path', how='inner')\n",
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered['patient_id'].unique().shape)\n",
    "print(filtered[filtered['years_to_cancer'] < 99]['patient_id'].unique().shape)\n",
    "print(filtered['exam_id'].unique().shape)\n",
    "for lower, upper in [(0, 40), (40, 50), (50, 60), (60, 70), (70, 80), (80, 200)]:\n",
    "    print(lower, upper)\n",
    "    age_grp = filtered[(filtered['age_at_study'] >= lower) \n",
    "                                & (filtered['age_at_study'] < upper)]\n",
    "    print(age_grp['exam_id'].unique().shape)\n",
    "    print(age_grp[age_grp['years_to_cancer'] < 99]['exam_id'].unique().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered.drop_duplicates(['patient_id']).groupby('ETHNICITY_DESC').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered['exam_id_y'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered['exam_id'] = filtered['exam_id_y']\n",
    "filtered['patient_id'] = filtered['patient_id_y']\n",
    "filtered['view'] = filtered['view_y']\n",
    "filtered['laterality'] = filtered['laterality_y']\n",
    "filtered['file_path'] = filtered['file_path_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Grab AsymMirai predictions, and figure out which (if any) are missing\n",
    "asym_preds = pd.read_csv('/PATH/TO/ASYMMIRAI/PREDICTIONS')\n",
    "print(asym_preds.columns)\n",
    "missing_asym_exams = filtered[~filtered['exam_id'].isin(asym_preds['exam_id'])]['exam_id'].unique()\n",
    "\n",
    "# If we're missing any, run AsymMirai and grab those results\n",
    "if missing_asym_exams.shape[0] > 0:\n",
    "    model = torch.load('./asymmetry_model/training_preds/full_model_epoch_40_3_11_corrected_flex.pt', \n",
    "                       map_location = torch.device(f'cuda:{6}'))\n",
    "    \n",
    "    #val_file_name = '../2_10_mirai_form_extended_cohorts_1-2_with_matches.csv'\n",
    "    val_df = filtered\n",
    "    print(val_df['exam_id'].unique().shape)\n",
    "    val_df = val_df[val_df['exam_id'].isin(missing_asym_exams)]\n",
    "    print(val_df['exam_id'].unique().shape)\n",
    "    \n",
    "    missing_preds = run_validation(model, val_df)\n",
    "    asym_preds = pd.concat([asym_preds, missing_preds])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Grab Mirai predictions, and figure out which (if any) are missing\n",
    "mirai_preds = pd.read_csv('/PATH/TO/MIRAI/PREDICTIONS', header=None)\n",
    "for i in range(5):\n",
    "    mirai_preds['year_{}_risk'.format(i+1)] = mirai_preds[4+i]\n",
    "\n",
    "'''\n",
    "Add exam ID to mirai's predictions\n",
    "'''\n",
    "def get_exam_id(row):\n",
    "    file_path = row[0]\n",
    "    return filtered[filtered['file_path'] == file_path]['exam_id'].values[0]\n",
    "mirai_preds['exam_id'] = mirai_preds.apply(get_exam_id, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check whether we're missing any exams\n",
    "missing_mirai_exams = filtered[~filtered['exam_id'].isin(mirai_preds['exam_id'])]['exam_id'].unique()\n",
    "\n",
    "# If we're missing any, run AsymMirai and grab those results\n",
    "if missing_mirai_exams.shape[0] > 0:\n",
    "    val_df = filtered\n",
    "    print(val_df['exam_id'].unique().shape)\n",
    "    val_df = val_df[val_df['exam_id'].isin(missing_mirai_exams)]\n",
    "    val_df['split_group'] = 'test'\n",
    "    val_df.loc[:, 'years_to_last_followup'] = 100\n",
    "    print(val_df['exam_id'].unique().shape)\n",
    "    \n",
    "    val_df[['exam_id','patient_id','laterality','view',\n",
    "           'file_path','years_to_cancer','years_to_last_followup',\n",
    "            'split_group']].to_csv('./tmp_val_input_for_mirai_2.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in val_df['years_to_cancer']:\n",
    "    print(int(float(val)))\n",
    "for val in val_df['years_to_last_followup']:\n",
    "    print(int(float(val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./scripts/main.py  --model_name mirai_full \\\n",
    "                        --img_encoder_snapshot ./snapshots/mgh_mammo_MIRAI_Base_May20_2019.p \\\n",
    "                        --transformer_snapshot ./snapshots/mgh_mammo_cancer_MIRAI_Transformer_Jan13_2020.p  \\\n",
    "                        --callibrator_snapshot ./snapshots/callibrators/MIRAI_FULL_PRED_RF.callibrator.p \\\n",
    "                        --batch_size 2 \\\n",
    "                        --dataset csv_mammo_risk_all_full_future \\\n",
    "                        --img_mean 7699.5 \\\n",
    "                        --img_size 2294 1914 \\\n",
    "                        --img_std 11765.06 \\\n",
    "                        --metadata_path ./tmp_val_input_for_mirai_2.csv\\\n",
    "                        --test \\\n",
    "                        --prediction_save_path ./tmp_val_prdictions_for_mirai.csv \\\n",
    "                        --results_path ./tmp_val_prdictions_for_mirai.csv \\\n",
    "                        --cuda \\\n",
    "                        --num_gpus 1 \\\n",
    "                        --test\n",
    "\n",
    " #../2_5_mirai_form_cohorts_1-2_no_diagnostic.csv \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine our newly fixed stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_filtered = filtered.merge(asym_preds, on='exam_id', suffixes=['', '_asym'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_filtered = merged_df_filtered.merge(mirai_preds, on='exam_id', suffixes=['', '_mirai'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_auc_for_asymm(probs, censor_times, golds, followup, calculate_curve=False):\n",
    "    def include_exam_and_determine_label(censor_time, gold):\n",
    "        valid_pos = gold and censor_time <= followup\n",
    "        valid_neg = censor_time >= followup\n",
    "        included, label = (valid_pos or valid_neg), valid_pos\n",
    "        return included, label\n",
    "\n",
    "    probs_for_eval, golds_for_eval = [], []\n",
    "    for prob_arr, censor_time, gold in zip(probs, censor_times, golds):\n",
    "        include, label = include_exam_and_determine_label(censor_time, gold)\n",
    "        if include:\n",
    "            probs_for_eval.append(prob_arr)\n",
    "            golds_for_eval.append(label)\n",
    "    try:\n",
    "        auc = sklearn.metrics.roc_auc_score(golds_for_eval, probs_for_eval, average='samples')\n",
    "        avg_precision = sklearn.metrics.average_precision_score(golds_for_eval, probs_for_eval, average='samples')\n",
    "        if calculate_curve:\n",
    "            fpr, tpr, thresh = sklearn.metrics.roc_curve(golds_for_eval, probs_for_eval)\n",
    "            plt.plot(fpr, tpr)\n",
    "            plt.title(fYear {followup + 1} Asymmetry ROC Curve\")\n",
    "            plt.show()\n",
    "    except Exception as e:\n",
    "        print(\"Failed to calculate AUC because {}\".format(e))\n",
    "        auc, avg_precision = ['NA']*2\n",
    "\n",
    "    return auc, avg_precision, golds_for_eval\n",
    "\n",
    "def get_label(row, max_followup=10, mode='censor_time'):\n",
    "    any_cancer = row[\"years_to_cancer\"] < max_followup\n",
    "    cancer_key = \"years_to_cancer\"\n",
    "\n",
    "    y =  any_cancer\n",
    "\n",
    "    if y:\n",
    "        censor_time = int(row[cancer_key])\n",
    "\n",
    "    else:\n",
    "        censor_time = int(min(row[\"years_to_last_followup\"], max_followup))# - 1)\n",
    "\n",
    "    #y_mask = np.array([1] * (censor_time+1) + [0]* (self.args.max_followup - (censor_time+1) ))\n",
    "    #assert len(y_mask) == self.args.max_followup\n",
    "    if mode == 'censor_time':\n",
    "        return censor_time\n",
    "    else:\n",
    "        return any_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_filtered['exam_id'].unique().shape\n",
    "res = merged_df_filtered.apply(get_label, args=(10, 'censor_time'), axis=1)\n",
    "merged_df_filtered['censor_time'] = res\n",
    "res = merged_df_filtered.apply(get_label, args=(10, 'any_cancer'), axis=1)\n",
    "merged_df_filtered['any_cancer'] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_filtered_simplified = merged_df_filtered.drop_duplicates(['exam_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_filtered_simplified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_filtered_copy = merged_df_filtered.copy()\n",
    "merged_df_filtered_copy.to_csv('tmp_merged_df_filtered_copy.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_filtered_copy = pd.read_csv('tmp_merged_df_filtered_copy.csv')\n",
    "merged_df_filtered_copy['prediction_pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arrs_for_auc(probs, censor_times, golds, followup):\n",
    "    \n",
    "    def include_exam_and_determine_label(censor_time, gold):\n",
    "        valid_pos = gold and censor_time <= followup\n",
    "        valid_neg = censor_time >= followup\n",
    "        included, label = (valid_pos or valid_neg), valid_pos\n",
    "        return included, label\n",
    "\n",
    "    probs_for_eval, golds_for_eval = [], []\n",
    "    for prob_arr, censor_time, gold in zip(probs, censor_times, golds):\n",
    "        include, label = include_exam_and_determine_label(censor_time, gold)\n",
    "        if include:\n",
    "            probs_for_eval.append(prob_arr)\n",
    "            golds_for_eval.append(label)\n",
    "    return probs_for_eval, golds_for_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyroc\n",
    "plt.style.use('seaborn-poster')\n",
    "legend = []\n",
    "for year in range(5):\n",
    "    probs_mirai, labels = get_arrs_for_auc(merged_df_filtered_simplified[f'year_{year+1}_risk'],\n",
    "                                     merged_df_filtered_simplified['censor_time'],\n",
    "                                     merged_df_filtered_simplified['any_cancer'],\n",
    "                                    year)\n",
    "    probs_asymmirai, labels = get_arrs_for_auc(merged_df_filtered_simplified[f'prediction_pos'],\n",
    "                                     merged_df_filtered_simplified['censor_time'],\n",
    "                                     merged_df_filtered_simplified['any_cancer'],\n",
    "                                    year)\n",
    "    for i, v in enumerate(probs_asymmirai):\n",
    "        if not type(v) is float:\n",
    "            probs_asymmirai[i] = v[1]\n",
    "    print(len(probs_asymmirai))\n",
    "    df = pd.DataFrame({\n",
    "        'AsymMirai': probs_asymmirai,\n",
    "        'Mirai': probs_mirai\n",
    "    })\n",
    "    roc = pyroc.ROC(labels,\n",
    "                    df)\n",
    "    \n",
    "    fpr, tpr, thresh = sklearn.metrics.roc_curve(labels, probs_asymmirai)\n",
    "    matplotlib.rc('xtick', labelsize=19) \n",
    "    matplotlib.rc('ytick', labelsize=19) \n",
    "    plt.rcParams.update({'axes.labelsize': 40})\n",
    "    \n",
    "    plt.plot(fpr, tpr)\n",
    "    auc = roc.auc[0, 1]\n",
    "    legend.append(\"Year {:.0f} AUC: {:.2f} ({:.2f}, {:.2f})\".format(\n",
    "        year+1, auc, roc.ci(0.05)[:, 1][0], roc.ci(0.05)[:, 1][1])\n",
    "     )\n",
    "    print(f'AsymMirai year {year + 1} 95% CI: \\t', roc.ci(0.05)[:, 0])\n",
    "    #print(f'Mirai year {year + 1} 95% CI: \\t\\t', roc.ci(0.05)[:, 1])\n",
    "plt.legend(legend, prop={'size': 20})\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#asym_risk_df['y_argmin'].values[0]\n",
    "import re\n",
    "\n",
    "def str_to_arr(arr_str):\n",
    "    #if type(arr_str) is not str:\n",
    "    #    return None\n",
    "    arr_str = re.sub('\\s+', ',', arr_str)\n",
    "    arr_str = arr_str.replace('[,', '[')\n",
    "    return np.array(eval(arr_str))\n",
    "\n",
    "def correct_y_argmin_cc(row):\n",
    "    if type(row['y_argmin_cc']) is float:\n",
    "        return int(row['y_argmin_cc'])\n",
    "    armin_arr = str_to_arr(row['y_argmin_cc'])\n",
    "    #if armin_arr is None:\n",
    "    #    return None\n",
    "    argmin = armin_arr[int(row['x_argmin_cc'])]\n",
    "    return argmin\n",
    "\n",
    "def correct_y_argmin_mlo(row):\n",
    "    if type(row['y_argmin_mlo']) is float:\n",
    "        return int(row['y_argmin_mlo'])\n",
    "    armin_arr = str_to_arr(row['y_argmin_mlo'])\n",
    "    #if armin_arr is None:\n",
    "    #    return None\n",
    "    argmin = armin_arr[int(row['x_argmin_mlo'])]\n",
    "    return argmin\n",
    "\n",
    "merged_df_filtered_simplified['y_argmin_cc'] = merged_df_filtered_simplified.apply(correct_y_argmin_cc, axis=1)\n",
    "merged_df_filtered_simplified['y_argmin_mlo'] = merged_df_filtered_simplified.apply(correct_y_argmin_mlo, axis=1)\n",
    "\n",
    "merged_df_filtered_simplified['mlo_y_argmin'] = merged_df_filtered_simplified['y_argmin_mlo']\n",
    "merged_df_filtered_simplified['mlo_x_argmin'] = merged_df_filtered_simplified['x_argmin_mlo']\n",
    "merged_df_filtered_simplified['cc_y_argmin'] = merged_df_filtered_simplified['y_argmin_cc']\n",
    "merged_df_filtered_simplified['cc_x_argmin'] = merged_df_filtered_simplified['x_argmin_cc']\n",
    "\n",
    "merged_df_filtered_simplified['asymmetries'] = merged_df_filtered_simplified['prediction_pos']\n",
    "merged_df_filtered_simplified['mlo_asym'] = merged_df_filtered_simplified['asymmetries']\n",
    "merged_df_filtered_simplified['cc_asym'] = merged_df_filtered_simplified['asymmetries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def safe_apply(fun, array):\n",
    "    if len(array) == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        val = fun(array)\n",
    "        if val == np.nan:\n",
    "            print(array, val, fun)\n",
    "        return val\n",
    "\n",
    "for pid in tqdm(merged_df_filtered_simplified['patient_id'].unique(), total=merged_df_filtered_simplified['patient_id'].nunique()):\n",
    "    cur_patient = merged_df_filtered_simplified[merged_df_filtered_simplified['patient_id'] == pid]\n",
    "    cur_patient = cur_patient.sort_values('years_to_last_followup', ascending=False)\n",
    "    \n",
    "    # For each exam with a following exam\n",
    "    for i in range(cur_patient.shape[0]):\n",
    "        mlo_shifts = []\n",
    "        cc_shifts = []\n",
    "        total_shifts = []\n",
    "        previous_exam_time_delta = []\n",
    "        \n",
    "        cur_exam = cur_patient[cur_patient['exam_id'] == cur_patient['exam_id'].unique()[i]]\n",
    "        prev_exams = cur_patient[cur_patient['years_to_last_followup'] > cur_exam['years_to_last_followup'].values[0]]\n",
    "        merged_df_filtered_simplified.loc[merged_df_filtered_simplified['exam_id'] == cur_exam['exam_id'].values[0], 'num_prev_exams'] = len(prev_exams['exam_id'].unique())\n",
    "\n",
    "        previous_exam_time_delta = np.inf\n",
    "        previous_exam_shift = np.nan\n",
    "\n",
    "        for cur_eid in prev_exams['exam_id'].unique():\n",
    "            e_other = prev_exams[prev_exams['exam_id'] == cur_eid]\n",
    "\n",
    "\n",
    "            old_x_mlo, old_y_mlo = cur_exam['x_argmin_mlo'].values[0], cur_exam['y_argmin_mlo'].values[0]\n",
    "            old_x_cc, old_y_cc = cur_exam['x_argmin_cc'].values[0], cur_exam['y_argmin_cc'].values[0]\n",
    "\n",
    "            new_x_mlo, new_y_mlo = e_other['x_argmin_mlo'].values[0], e_other['y_argmin_mlo'].values[0]\n",
    "            new_x_cc, new_y_cc = e_other['x_argmin_cc'].values[0], e_other['y_argmin_cc'].values[0]\n",
    "\n",
    "            mlo_shift = ((old_x_mlo - new_x_mlo) ** 2 + (old_y_mlo - new_y_mlo) ** 2) ** 0.5\n",
    "            mlo_shifts.append(mlo_shift)\n",
    "            \n",
    "            cc_shift = ((old_x_cc - new_x_cc) ** 2 + (old_y_cc - new_y_cc) ** 2) ** 0.5\n",
    "            cc_shifts.append(cc_shift)\n",
    "            \n",
    "            total_shift = (mlo_shift ** 2 + cc_shift ** 2) ** 0.5\n",
    "            total_shifts.append(total_shift)\n",
    "\n",
    "            this_exam_time_delta = (e_other['years_to_last_followup'].item() - cur_exam['years_to_last_followup'].item()) * 12\n",
    "\n",
    "            if this_exam_time_delta < previous_exam_time_delta:\n",
    "                previous_exam_time_delta = this_exam_time_delta\n",
    "                previous_exam_shift = total_shift\n",
    "                \n",
    "        exam_query = merged_df_filtered_simplified['exam_id'] == cur_exam['exam_id'].values[0]\n",
    "        merged_df_filtered_simplified.loc[exam_query, 'centroid_mlo_shift_med'] = safe_apply(np.median, mlo_shifts)\n",
    "        merged_df_filtered_simplified.loc[exam_query, 'centroid_cc_shift_med'] = safe_apply(np.median, cc_shifts)\n",
    "        merged_df_filtered_simplified.loc[exam_query, 'centroid_total_shift_med'] = safe_apply(np.median, total_shifts)\n",
    "\n",
    "        merged_df_filtered_simplified.loc[exam_query, 'centroid_mlo_shift_mean'] = safe_apply(np.mean, mlo_shifts)\n",
    "        merged_df_filtered_simplified.loc[exam_query, 'centroid_cc_shift_mean'] = safe_apply(np.mean, cc_shifts)\n",
    "        merged_df_filtered_simplified.loc[exam_query, 'centroid_total_shift_mean'] = safe_apply(np.mean, total_shifts)\n",
    "\n",
    "        merged_df_filtered_simplified.loc[exam_query, 'centroid_total_shifts'] = np.nan if len(total_shifts) == 0 else str(total_shifts)\n",
    "\n",
    "        merged_df_filtered_simplified.loc[exam_query, 'centroid_total_shift_min'] = safe_apply(np.min, total_shifts)\n",
    "        merged_df_filtered_simplified.loc[exam_query, 'centroid_total_shift_max'] = safe_apply(np.max, total_shifts)\n",
    "        merged_df_filtered_simplified.loc[exam_query, 'centroid_total_shift_from_last'] = previous_exam_shift\n",
    "        merged_df_filtered_simplified.loc[exam_query, 'prev_exam_time_delta_mon'] = previous_exam_time_delta\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def safe_apply(fun, array):\n",
    "    if len(array) == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        val = fun(array)\n",
    "        if val == np.nan:\n",
    "            print(array, val, fun)\n",
    "        return val\n",
    "\n",
    "for pid in tqdm(merged_df_filtered_simplified['patient_id'].unique(), total=merged_df_filtered_simplified['patient_id'].nunique()):\n",
    "    cur_patient = merged_df_filtered_simplified[merged_df_filtered_simplified['patient_id'] == pid]\n",
    "    cur_patient = cur_patient.sort_values('years_to_last_followup', ascending=False)\n",
    "    \n",
    "    # For each exam with a following exam\n",
    "    for i in range(cur_patient.shape[0]):\n",
    "        year_1_deltas = []\n",
    "        year_2_deltas = []\n",
    "        year_3_deltas = []\n",
    "        year_4_deltas = []\n",
    "        year_5_deltas = []\n",
    "        asym_deltas = []\n",
    "        previous_exam_time_delta = []\n",
    "        \n",
    "        cur_exam = cur_patient[cur_patient['exam_id'] == cur_patient['exam_id'].unique()[i]]\n",
    "        prev_exams = cur_patient[cur_patient['years_to_last_followup'] > cur_exam['years_to_last_followup'].values[0]]\n",
    "        merged_df_filtered_simplified.loc[merged_df_filtered_simplified['exam_id'] == cur_exam['exam_id'].values[0], 'num_prev_exams'] = len(prev_exams['exam_id'].unique())\n",
    "\n",
    "        previous_exam_time_delta = np.inf\n",
    "        previous_exam_shift = np.nan\n",
    "        year_1_delta = np.nan\n",
    "        year_2_delta = np.nan\n",
    "        year_3_delta = np.nan\n",
    "        year_4_delta = np.nan\n",
    "        year_5_delta = np.nan\n",
    "        asym_delta = np.nan\n",
    "\n",
    "        for cur_eid in prev_exams['exam_id'].unique():\n",
    "            e_other = prev_exams[prev_exams['exam_id'] == cur_eid]\n",
    "\n",
    "            this_exam_time_delta = (e_other['years_to_last_followup'].item() - cur_exam['years_to_last_followup'].item()) * 12\n",
    "            \n",
    "            year_1_deltas.append(abs(e_other['year_1_risk'].item() - cur_exam['year_1_risk'].item()))\n",
    "            year_2_deltas.append(abs(e_other['year_2_risk'].item() - cur_exam['year_2_risk'].item()))\n",
    "            year_3_deltas.append(abs(e_other['year_3_risk'].item() - cur_exam['year_3_risk'].item()))\n",
    "            year_4_deltas.append(abs(e_other['year_4_risk'].item() - cur_exam['year_4_risk'].item()))\n",
    "            year_5_deltas.append(abs(e_other['year_5_risk'].item() - cur_exam['year_5_risk'].item()))\n",
    "            asym_deltas.append(abs(e_other['asymmetries'].item() - cur_exam['asymmetries'].item()))\n",
    "\n",
    "            if this_exam_time_delta < previous_exam_time_delta:\n",
    "\n",
    "                year_1_delta = e_other['year_1_risk'].item() - cur_exam['year_1_risk'].item()\n",
    "                year_2_delta = e_other['year_2_risk'].item() - cur_exam['year_2_risk'].item()\n",
    "                year_3_delta = e_other['year_3_risk'].item() - cur_exam['year_3_risk'].item()\n",
    "                year_4_delta = e_other['year_4_risk'].item() - cur_exam['year_4_risk'].item()\n",
    "                year_5_delta = e_other['year_5_risk'].item() - cur_exam['year_5_risk'].item()\n",
    "                asym_delta = e_other['asymmetries'].item() - cur_exam['asymmetries'].item()\n",
    "                \n",
    "                previous_exam_time_delta = this_exam_time_delta\n",
    "                \n",
    "        exam_query = merged_df_filtered_simplified['exam_id'] == cur_exam['exam_id'].values[0]\n",
    "        merged_df_filtered_simplified.loc[exam_query, 'year_1_risk_delta'] = year_1_delta\n",
    "        merged_df_filtered_simplified.loc[exam_query, 'year_2_risk_delta'] = year_2_delta\n",
    "        merged_df_filtered_simplified.loc[exam_query, 'year_3_risk_delta'] = year_3_delta\n",
    "        merged_df_filtered_simplified.loc[exam_query, 'year_4_risk_delta'] = year_4_delta\n",
    "        merged_df_filtered_simplified.loc[exam_query, 'year_5_risk_delta'] = year_5_delta\n",
    "        merged_df_filtered_simplified.loc[exam_query, 'asymmetries'] = asym_delta\n",
    "        \n",
    "        merged_df_filtered_simplified.loc[exam_query, 'year_1_risk_delta_med'] = safe_apply(np.median, year_1_deltas)\n",
    "        merged_df_filtered_simplified.loc[exam_query, 'year_2_risk_delta_med'] = safe_apply(np.median, year_2_deltas)\n",
    "        merged_df_filtered_simplified.loc[exam_query, 'year_3_risk_delta_med'] = safe_apply(np.median, year_3_deltas)\n",
    "        merged_df_filtered_simplified.loc[exam_query, 'year_4_risk_delta_med'] = safe_apply(np.median, year_4_deltas)\n",
    "        merged_df_filtered_simplified.loc[exam_query, 'year_5_risk_delta_med'] = safe_apply(np.median, year_5_deltas)\n",
    "        merged_df_filtered_simplified.loc[exam_query, 'asymmetries_med'] = safe_apply(np.median, asym_deltas)\n",
    "        \n",
    "        merged_df_filtered_simplified.loc[exam_query, 'year_1_risk_delta_mean'] = safe_apply(np.mean, year_1_deltas)\n",
    "        merged_df_filtered_simplified.loc[exam_query, 'year_2_risk_delta_mean'] = safe_apply(np.mean, year_2_deltas)\n",
    "        merged_df_filtered_simplified.loc[exam_query, 'year_3_risk_delta_mean'] = safe_apply(np.mean, year_3_deltas)\n",
    "        merged_df_filtered_simplified.loc[exam_query, 'year_4_risk_delta_mean'] = safe_apply(np.mean, year_4_deltas)\n",
    "        merged_df_filtered_simplified.loc[exam_query, 'year_5_risk_delta_mean'] = safe_apply(np.mean, year_5_deltas)\n",
    "        merged_df_filtered_simplified.loc[exam_query, 'asymmetries_mean'] = safe_apply(np.mean, asym_deltas)\n",
    "        \n",
    "        merged_df_filtered_simplified.loc[exam_query, 'prev_exam_time_delta_mon'] = previous_exam_time_delta\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_filtered_simplified['centroid_total_shift_from_last']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asyms = []\n",
    "predictions_pos = []\n",
    "predictions_neg = []\n",
    "for i, v in enumerate(merged_df_filtered_simplified['asymmetries']):\n",
    "    if type(v) is np.ndarray:\n",
    "        asyms.append(float(v[1]))\n",
    "        predictions_pos.append(float(v[1]))\n",
    "        predictions_neg.append(float(v[0]))\n",
    "    else:\n",
    "        asyms.append(v)\n",
    "        predictions_pos.append(v)\n",
    "        predictions_neg.append(1 - v)\n",
    "                               \n",
    "merged_df_filtered_simplified.loc[:, 'asymmetries'] = asyms\n",
    "merged_df_filtered_simplified.loc[:, 'prediction_pos'] = predictions_pos\n",
    "merged_df_filtered_simplified.loc[:, 'prediction_neg'] = predictions_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.metrics\n",
    "asym_location_aucs = {}\n",
    "year=5\n",
    "asym_loc_stats = ('med', 'from_last', 'mean')#'min', 'max', 'mean', 'from_last')\n",
    "for stat in asym_loc_stats: \n",
    "    aucs_asym = []\n",
    "    thresholds = []\n",
    "    included_exams = []\n",
    "    included_patients = []\n",
    "    cancer_patients = []\n",
    "    ci_low = []\n",
    "    ci_high = []\n",
    "\n",
    "    col = f'centroid_total_shift_{stat}'\n",
    "\n",
    "    run_size = len(merged_df_filtered_simplified[col].unique())\n",
    "    for ind, delta_thresh in tqdm(enumerate(merged_df_filtered_simplified.sort_values(col)[col].unique()[:-1]), total=run_size):\n",
    "\n",
    "        if col.startswith('from_last'):\n",
    "            query = (merged_df_filtered_simplified[col] <= delta_thresh) & (merged_df_filtered_simplified['prev_exam_time_delta_mon'] <= 18)\n",
    "        else:\n",
    "            query = merged_df_filtered_simplified[col] <= delta_thresh\n",
    "\n",
    "        subset = merged_df_filtered_simplified[query]\n",
    "        \n",
    "        probs_asymmirai, labels = get_arrs_for_auc(subset[f'prediction_pos'],\n",
    "                                         subset['censor_time'],\n",
    "                                         subset['any_cancer'],\n",
    "                                        year)\n",
    "        try:\n",
    "            df = pd.DataFrame({\n",
    "            'AsymMirai': probs_asymmirai,\n",
    "            'Mirai': probs_asymmirai\n",
    "            })\n",
    "            roc = pyroc.ROC(labels, df)\n",
    "            \n",
    "            auc = roc.auc[0, 1]\n",
    "            ci_low.append(roc.ci(0.05)[0, 0])\n",
    "            ci_high.append(roc.ci(0.05)[1, 0])\n",
    "        except:\n",
    "            auc, avg_precision, include = compute_auc_for_asymm(subset[f'asymmetries'].values, \n",
    "                                subset['censor_time'].values, \n",
    "                                subset['any_cancer'].values, year)\n",
    "            ci_low.append(-1)\n",
    "            ci_high.append(2)\n",
    "            \n",
    "        aucs_asym.append(auc)\n",
    "        thresholds.append(delta_thresh)\n",
    "        included_exams.append(subset['exam_id'].nunique())\n",
    "        included_patients.append(subset['patient_id'].nunique())\n",
    "        cancer_patients.append(subset[subset['any_cancer']]['patient_id'].nunique())\n",
    "    \n",
    "    asym_location_aucs[stat] = {'auc': aucs_asym, 'thresh': thresholds, \n",
    "                                'exam': included_exams, 'patient': included_patients,\n",
    "                                'cancer_patients': cancer_patients,\n",
    "                                'ci_low': ci_low, 'ci_high': ci_high}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_dfs = {}\n",
    "\n",
    "auc_dfs.update({f'centroid-asym-loc-{stat}': pd.DataFrame.from_dict(asym_location_aucs[stat]) for stat in asym_loc_stats})\n",
    "[(f'{stat}-{field}', len(asym_location_aucs[stat][field])) for stat in asym_loc_stats for field in ('auc', 'thresh', 'exam', 'patient', 'cancer_patients', 'ci_low', 'ci_high')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(auc_dfs).to_csv('6_28_auc_dfs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hex_to_tuple(color, alpha=0.2):\n",
    "    color = color[1:]\n",
    "    return tuple([int(color[i:i+2], 16) / 255 for i in (0, 2, 4)] + [alpha])\n",
    "prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "colors = prop_cycle.by_key()['color']\n",
    "colors_in_tuple = [hex_to_tuple(c) for c in colors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend = list(auc_dfs.keys())\n",
    "for x_axis in ['patient', 'thresh']:\n",
    "    for i, metric in enumerate(['centroid-asym-loc-from_last']):\n",
    "        \n",
    "        plt.errorbar(auc_dfs[metric][x_axis][3:-1] if 'thresh' not in x_axis else auc_dfs[metric][x_axis][3:-1] * 5/64 * 100, \n",
    "                 auc_dfs[metric]['auc'][3:-1],\n",
    "                auc_dfs[metric]['ci_high'][3:-1] - auc_dfs[metric]['auc'][3:-1],\n",
    "                ecolor=colors_in_tuple[i],\n",
    "                elinewidth=1)\n",
    "        \n",
    "    matplotlib.rc('xtick', labelsize=19) \n",
    "    matplotlib.rc('ytick', labelsize=19) \n",
    "    plt.rcParams.update({'axes.labelsize': 40})\n",
    "    \n",
    "    if 'patient' in x_axis:\n",
    "        plt.xlabel('# Patients Included')\n",
    "        plt.legend(['Consistency with Previous Exam'])\n",
    "    else:\n",
    "        plt.xlabel('Maximum Window Shift %')\n",
    "        plt.xticks([50, 250, 450, 650, 850])\n",
    "        plt.legend(['Consistency with Previous Exam'])\n",
    "        plt.axvline(50, linestyle='dashed', c='black')\n",
    "        \n",
    "    plt.ylabel(\"AUC\")\n",
    "    plt.savefig(f'./consistenct_auc_by_{x_axis}.png', dpi=300)\n",
    "    plt.clf()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from scipy import stats\n",
    "patient_count_dict = {}\n",
    "for i, metric in enumerate(['centroid_total_shift_from_last', 'centroid_total_shift_med', 'centroid_total_shift_mean']):\n",
    "    patient_count_dict[metric] = []\n",
    "    legend = []\n",
    "    \n",
    "    # Convert percentages (40, 50, ...) to a shift proportion\n",
    "    for shift_val in [i / (5/64 * 100) for i in [40, 50, 60, 80, 100]]:\n",
    "        subset = merged_df_filtered_simplified[merged_df_filtered_simplified[metric] <= shift_val]\n",
    "        patient_count_dict[metric].append(subset['patient_id'].unique().shape[0])\n",
    "    \n",
    "        \n",
    "        probs_asymmirai, labels = get_arrs_for_auc(subset[f'prediction_pos'],\n",
    "                                             subset['censor_time'],\n",
    "                                             subset['any_cancer'],\n",
    "                                            year)\n",
    "        df = pd.DataFrame({\n",
    "            'AsymMirai': probs_asymmirai,\n",
    "            'Mirai': probs_asymmirai\n",
    "        })\n",
    "        roc = pyroc.ROC(labels,\n",
    "                        df)#merged_df_w_demographics[screen][valid][['prediction_pos', f'year_{year+1}_risk']])\n",
    "\n",
    "        \n",
    "        fpr, tpr, thresh = sklearn.metrics.roc_curve(labels, probs_asymmirai)\n",
    "        legend.append(f'''Max {shift_val * (5/64 * 100)}% shift\n",
    "{round(roc.auc[0,0], 2)} AUC ({round(roc.ci(0.05)[0, 0], 2)}, {round(roc.ci(0.05)[1, 0], 2)}), {subset['patient_id'].unique().shape[0]} Patients''')\n",
    "        \n",
    "        plt.plot(fpr, tpr)\n",
    "    plt.xlabel('FPR')\n",
    "\n",
    "    plt.rcParams.update({'axes.labelsize': 4})\n",
    "\n",
    "    plt.ylabel(\"TPR\")\n",
    "    plt.legend(legend, prop={'size': 20})\n",
    "    plt.show()\n",
    "    plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "for i, metric in enumerate(['year_5_risk_delta', 'year_5_risk_delta_med', 'year_5_risk_delta_mean']):\n",
    "    legend = []\n",
    "    if 'mean' in metric:\n",
    "        patients = patient_count_dict['centroid_total_shift_mean']\n",
    "    elif 'med' in metric:\n",
    "        patients = patient_count_dict['centroid_total_shift_med']\n",
    "    else:\n",
    "        patients = patient_count_dict['centroid_total_shift_from_last']\n",
    "        \n",
    "    risk_threshes = []\n",
    "    seen_counts = []\n",
    "    for v in merged_df_filtered_simplified[metric].abs().sort_values():\n",
    "        cur_p_count = merged_df_filtered_simplified[merged_df_filtered_simplified[metric].abs() <= v]['patient_id'].unique().shape[0]\n",
    "        if cur_p_count in patients and (cur_p_count not in seen_counts):\n",
    "            risk_threshes.append(v)\n",
    "            seen_counts.append(cur_p_count)\n",
    "            print(risk_threshes)\n",
    "        if len(risk_threshes) == len(patients):\n",
    "            break\n",
    "    for risk_thresh in risk_threshes:\n",
    "        subset = merged_df_filtered_simplified[merged_df_filtered_simplified[metric].abs() <= risk_thresh]\n",
    "        print(subset['patient_id'].unique().shape)\n",
    "        probs_mirai, labels = get_arrs_for_auc(subset[f'year_5_risk'],\n",
    "                                             subset['censor_time'],\n",
    "                                             subset['any_cancer'],\n",
    "                                            year)\n",
    "        df = pd.DataFrame({\n",
    "            'Mirai': probs_mirai\n",
    "        })\n",
    "        roc = pyroc.ROC(labels,\n",
    "                        df)#merged_df_w_demographics[screen][valid][['prediction_pos', f'year_{year+1}_risk']])\n",
    "\n",
    "        \n",
    "        fpr, tpr, thresh = sklearn.metrics.roc_curve(labels, probs_mirai)\n",
    "        legend.append(f'''Max Risk Change {round(risk_thresh, 3)}\n",
    "{round(roc.auc[0,0], 2)} AUC ({round(roc.ci(0.05)[0, 0], 2)}, {round(roc.ci(0.05)[1, 0], 2)}), {subset['patient_id'].unique().shape[0]} Patients''')\n",
    "        \n",
    "        plt.plot(fpr, tpr)\n",
    "    plt.xlabel('FPR')\n",
    "\n",
    "    plt.rcParams.update({'axes.labelsize': 4})\n",
    "\n",
    "    plt.ylabel(\"TPR\")\n",
    "    plt.legend(legend, prop={'size': 20})\n",
    "    #plt.savefig(f'./consistency_analysis_selected_curves.png', dpi=300)\n",
    "    plt.show()\n",
    "    plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "legend = []\n",
    "for shift_val in [i / (5/64 * 100) for i in [40, 50, 60, 80, 100]]:\n",
    "    for i, metric in enumerate(['centroid-asym-loc-from_last']):\n",
    "        \n",
    "        subset = merged_df_filtered_simplified[merged_df_filtered_simplified['centroid_total_shift_from_last'] <= shift_val]\n",
    "        \n",
    "        bootstrap_aucs = []\n",
    "        n_bootstraps = 2000\n",
    "        for _ in tqdm(range(n_bootstraps)):\n",
    "            resampled_subset = subset.sample(frac=1, replace=True)\n",
    "            probs_asymmirai, labels = get_arrs_for_auc(resampled_subset[f'prediction_pos'],\n",
    "                                                 resampled_subset['censor_time'],\n",
    "                                                 resampled_subset['any_cancer'],\n",
    "                                                year)\n",
    "            labels = 1 - np.array(labels)\n",
    "            precision, recall, thresholds = sklearn.metrics.precision_recall_curve(labels, probs_asymmirai)\n",
    "            auc = sklearn.metrics.auc(recall, precision)\n",
    "            bootstrap_aucs.append(auc)\n",
    "        bootstrap_aucs = pd.Series(bootstrap_aucs)\n",
    "        \n",
    "        probs_asymmirai, labels = get_arrs_for_auc(subset[f'prediction_pos'],\n",
    "                                             subset['censor_time'],\n",
    "                                             subset['any_cancer'],\n",
    "                                            year)\n",
    "        labels = 1 - np.array(labels)\n",
    "        precision, recall, thresholds = sklearn.metrics.precision_recall_curve(labels, probs_asymmirai)\n",
    "        auc = sklearn.metrics.auc(recall, precision)\n",
    "        legend.append(f'''Max {shift_val * (5/64 * 100)}% shift\n",
    "{round(auc, 2)} AUC ({round(bootstrap_aucs.quantile(0.025), 2)}, {round(bootstrap_aucs.quantile(0.975), 2)}), {subset['patient_id'].unique().shape[0]} Patients''')\n",
    "        \n",
    "        plt.plot(recall, precision)\n",
    "plt.xlabel('FPR')\n",
    "\n",
    "plt.rcParams.update({'axes.labelsize': 4})\n",
    "\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.legend(legend, prop={'size': 20})\n",
    "#plt.savefig(f'./consistency_analysis_selected_curves.png', dpi=300)\n",
    "plt.show()\n",
    "plt.clf()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_filtered_simplified\n",
    "subset = merged_df_filtered_simplified[(merged_df_filtered_simplified['ETHNICITY_DESC'] != 'Caucasian or White')\n",
    "                                      & (merged_df_filtered_simplified['ETHNICITY_DESC'] != 'African American  or Black')]\n",
    "print(subset.shape)\n",
    "for year in range(5):\n",
    "    probs_asymmirai, labels = get_arrs_for_auc(subset[f'prediction_pos'],\n",
    "                                         subset['censor_time'],\n",
    "                                         subset['any_cancer'],\n",
    "                                        year)\n",
    "    df = pd.DataFrame({\n",
    "        'AsymMirai': probs_asymmirai\n",
    "    })\n",
    "    roc = pyroc.ROC(labels,\n",
    "                    df)#merged_df_w_demographics[screen][valid][['prediction_pos', f'year_{year+1}_risk']])\n",
    "\n",
    "    print(f\"YEAR {year + 1}\")\n",
    "    print(\"AUCS\", roc.auc[0, 0])\n",
    "    fpr, tpr, thresh = sklearn.metrics.roc_curve(labels, probs_asymmirai)\n",
    "    auc = roc.auc[0, 1]\n",
    "    legend.append(\"Year {:.0f} AUC: {:.3f} (±{:.3f})\".format(\n",
    "        year+1, auc, roc.ci(0.05)[:, 1][1] - auc)\n",
    "     )\n",
    "    print(f'95% CI: \\t', roc.ci(0.05)[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_filtered_simplified['ETHNICITY_DESC'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_filtered_simplified['ETHNIC_GROUP_DESC'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mirai_env",
   "language": "python",
   "name": "mirai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
